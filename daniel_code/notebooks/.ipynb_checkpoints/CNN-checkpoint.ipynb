{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a CNN on LSE cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import tables\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import threading\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape, Input, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PARAMETERS\n",
    "# =============================================================================\n",
    "labels_of_interest = [\"heartAgeDataGender\"]  #[\"heartCondition\"]\n",
    "\n",
    "#File locations\n",
    "output_dir = \"/scratch/PI/euan/projects/mhc/code/daniel_code/results\"\n",
    "label_table_file = \"/scratch/PI/euan/projects/mhc/code/daniel_code/tables/combined_health_label_table.pkl\"\n",
    "label_table_file = \"/scratch/PI/euan/projects/mhc/code/daniel_code/tables/demographics_summary_v2.sex.tsv\"\n",
    "train_data_path = r\"/scratch/PI/euan/projects/mhc/code/daniel_code/data/cycles.hdf5\"\n",
    "\n",
    "#Training metrics\n",
    "from concise.metrics import tpr, tnr, fpr, fnr, precision, f1\n",
    "model_metrics = ['accuracy',tpr,tnr,fpr,fnr,precision,f1]\n",
    "\n",
    "#Training parameters\n",
    "batch_size = 256\n",
    "canMultiprocess = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/PI/euan/projects/mhc/code/daniel_code/data/cycles.hdf5 (File) 'cycles'\n",
      "Last modif.: 'Sat Jun 29 23:18:00 2019'\n",
      "Object Tree: \n",
      "/ (RootGroup) 'cycles'\n",
      "/data (EArray(32006, 100, 4)) ''\n",
      "/labels (EArray(32006,)) ''\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Look at what the data file looks like\n",
    "with tables.open_file(train_data_path, mode='r') as file:\n",
    "    print(file)\n",
    "    \n",
    "    #Test various data access schemes\n",
    "    import timeit\n",
    "    \n",
    "    idxs = np.random.randint(0, 32006, size=(320,))\n",
    "        \n",
    "    #timeit.timeit(stmt='print(file.root.data[:50])', number=10)\n",
    "    #%timeit e = [file.root.data[i] for i in range(320)]\n",
    "    #print(idxs)\n",
    "    #%timeit e = [file.root.data[i] for i in idxs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subject</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>002e538e-165f-45a4-ae19-d9d3538f6f66</th>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>002ee208-68c4-4bff-a561-e79450e4b0c6</th>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00478175-c1ba-4616-bda8-1551671a402d</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>005a0d08-fe21-4a24-9c57-791430c05f04</th>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>007ae3af-aec8-45bf-985d-7f7024574dd3</th>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>008c4978-514f-40b8-9fcb-5fe7b74fb708</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0098a0d0-3894-4218-9ef7-a91af52dffd2</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>009be6bd-8fdf-4bba-a272-560f8cf6c992</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00a148c2-a28c-4db5-9e48-31354c74ca20</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00b8f774-0822-43db-90a8-ba9a0a685a5e</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00da1a61-6ecc-4e8f-b106-029a7e8ce32f</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00e5bc52-26be-4286-a163-9b0fdf19cd9a</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00e77a9a-4b6a-4e61-a0e5-e86e946810af</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00f05777-bc8e-4900-89af-cc6732ccb69f</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00f4a2b3-97c9-447c-ba79-3afeb4fb655a</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01057f41-1716-4528-bb4a-fcbe0ed8104d</th>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0108dedd-2159-434b-831f-ca68b8b2bff5</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>010f2c15-0e7c-42f2-b083-0882b16d2454</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>011e788d-1e88-4422-b028-0cb954308eb7</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>014de7aa-85f7-44c7-9f40-e19538140541</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0152374d-8f90-4287-83b4-4a0ec0f301f2</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>015b123b-777f-4770-9f8a-df4d48a5bb23</th>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>015e1823-63c9-4530-8f59-65b1aedc867c</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>015e9bc0-6b9b-4963-8eff-0db7d1ba045b</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>016300f6-5473-43ef-a5cd-343c5ff3e3ba</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>016aee52-0d13-47f2-a2d1-cb8f83b640ed</th>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>016ec0ce-d544-4e9d-bea3-debcb5b280fa</th>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>017e26c9-4a9a-4963-91d7-bd48b51fd20f</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01875bbb-2bd9-46d4-acff-1e159f9a2ffb</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>018be265-7fd2-408a-b35b-14c44dafb904</th>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fe4a93d1-f3fd-486b-ac73-af3199c3ed4d</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fe71f237-f9a0-4cab-9a96-aab88f6c4b3c</th>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fea6e19c-fe93-4b58-93c6-bbb66ed24162</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feaa218b-70b4-4575-92a5-379b3e3297bc</th>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fec21464-5eb7-4c65-bc7c-7cbff43d2850</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fedb6bb9-bfdf-48b2-9855-07825d97fe37</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fee1ee53-d07c-4ef5-83bf-a58d2aa05980</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fee4e905-3c3a-4ea2-8c3a-f7a5339c93b3</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fee6d94d-79dd-4242-bec5-4cf3509f553e</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ff00076d-34a8-4c42-bec1-d214d93cf166</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ff041f69-e991-4c8e-a152-d8b17e5d2d80</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ff182063-510a-4c9a-9b90-6a7ed6c49091</th>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ff18f8fd-8a6f-4441-b02d-c5e9460a0de6</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ff194e5b-97ad-44e0-9d7b-3a4e25dc4779</th>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ff2e3771-30c2-4866-a2c2-ae829c2b3dee</th>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ff489c8a-f5ff-4f00-9682-33c44df02621</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ff5b5bb8-420e-4836-8aa8-2d6667972e2a</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ff663cca-2877-48a9-a6d8-8ddb451f31d5</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ff7233a2-977a-4e9b-bcf4-91e3c547c783</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ff8a70b8-f5f6-4749-95fb-3f7ad88dd40c</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ff9cdeed-bb7d-4c72-bc8d-7ea7252b5d92</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffaef5b8-a036-491d-b674-d91a9f83adeb</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffaff826-0602-4e7f-99aa-a6de6f16d2ce</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffbddae6-9e2f-4b16-9d79-6da19826bc84</th>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffc05de3-4806-484d-a7e5-c1931aba53f4</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffc1890c-8a22-44cf-927c-a7f203dac13c</th>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffd2313f-4be9-4ed1-b275-52a2d9a91fff</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffe6fcef-479a-4fab-b70c-07468b8f4c00</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffed63d3-f353-42a5-a290-06a102b03483</th>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fff93ef4-c4d5-4812-a30c-211588f742d8</th>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5332 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Sex\n",
       "Subject                                     \n",
       "002e538e-165f-45a4-ae19-d9d3538f6f66  Female\n",
       "002ee208-68c4-4bff-a561-e79450e4b0c6  Female\n",
       "00478175-c1ba-4616-bda8-1551671a402d    Male\n",
       "005a0d08-fe21-4a24-9c57-791430c05f04  Female\n",
       "007ae3af-aec8-45bf-985d-7f7024574dd3  Female\n",
       "008c4978-514f-40b8-9fcb-5fe7b74fb708    Male\n",
       "0098a0d0-3894-4218-9ef7-a91af52dffd2    Male\n",
       "009be6bd-8fdf-4bba-a272-560f8cf6c992    Male\n",
       "00a148c2-a28c-4db5-9e48-31354c74ca20    Male\n",
       "00b8f774-0822-43db-90a8-ba9a0a685a5e    Male\n",
       "00da1a61-6ecc-4e8f-b106-029a7e8ce32f    Male\n",
       "00e5bc52-26be-4286-a163-9b0fdf19cd9a    Male\n",
       "00e77a9a-4b6a-4e61-a0e5-e86e946810af    Male\n",
       "00f05777-bc8e-4900-89af-cc6732ccb69f    Male\n",
       "00f4a2b3-97c9-447c-ba79-3afeb4fb655a    Male\n",
       "01057f41-1716-4528-bb4a-fcbe0ed8104d  Female\n",
       "0108dedd-2159-434b-831f-ca68b8b2bff5    Male\n",
       "010f2c15-0e7c-42f2-b083-0882b16d2454    Male\n",
       "011e788d-1e88-4422-b028-0cb954308eb7    Male\n",
       "014de7aa-85f7-44c7-9f40-e19538140541    Male\n",
       "0152374d-8f90-4287-83b4-4a0ec0f301f2    Male\n",
       "015b123b-777f-4770-9f8a-df4d48a5bb23  Female\n",
       "015e1823-63c9-4530-8f59-65b1aedc867c    Male\n",
       "015e9bc0-6b9b-4963-8eff-0db7d1ba045b    Male\n",
       "016300f6-5473-43ef-a5cd-343c5ff3e3ba    Male\n",
       "016aee52-0d13-47f2-a2d1-cb8f83b640ed  Female\n",
       "016ec0ce-d544-4e9d-bea3-debcb5b280fa  Female\n",
       "017e26c9-4a9a-4963-91d7-bd48b51fd20f    Male\n",
       "01875bbb-2bd9-46d4-acff-1e159f9a2ffb    Male\n",
       "018be265-7fd2-408a-b35b-14c44dafb904  Female\n",
       "...                                      ...\n",
       "fe4a93d1-f3fd-486b-ac73-af3199c3ed4d    Male\n",
       "fe71f237-f9a0-4cab-9a96-aab88f6c4b3c  Female\n",
       "fea6e19c-fe93-4b58-93c6-bbb66ed24162    Male\n",
       "feaa218b-70b4-4575-92a5-379b3e3297bc  Female\n",
       "fec21464-5eb7-4c65-bc7c-7cbff43d2850    Male\n",
       "fedb6bb9-bfdf-48b2-9855-07825d97fe37    Male\n",
       "fee1ee53-d07c-4ef5-83bf-a58d2aa05980    Male\n",
       "fee4e905-3c3a-4ea2-8c3a-f7a5339c93b3    Male\n",
       "fee6d94d-79dd-4242-bec5-4cf3509f553e    Male\n",
       "ff00076d-34a8-4c42-bec1-d214d93cf166    Male\n",
       "ff041f69-e991-4c8e-a152-d8b17e5d2d80    Male\n",
       "ff182063-510a-4c9a-9b90-6a7ed6c49091  Female\n",
       "ff18f8fd-8a6f-4441-b02d-c5e9460a0de6    Male\n",
       "ff194e5b-97ad-44e0-9d7b-3a4e25dc4779  Female\n",
       "ff2e3771-30c2-4866-a2c2-ae829c2b3dee  Female\n",
       "ff489c8a-f5ff-4f00-9682-33c44df02621    Male\n",
       "ff5b5bb8-420e-4836-8aa8-2d6667972e2a    Male\n",
       "ff663cca-2877-48a9-a6d8-8ddb451f31d5    Male\n",
       "ff7233a2-977a-4e9b-bcf4-91e3c547c783    Male\n",
       "ff8a70b8-f5f6-4749-95fb-3f7ad88dd40c    Male\n",
       "ff9cdeed-bb7d-4c72-bc8d-7ea7252b5d92    Male\n",
       "ffaef5b8-a036-491d-b674-d91a9f83adeb    Male\n",
       "ffaff826-0602-4e7f-99aa-a6de6f16d2ce    Male\n",
       "ffbddae6-9e2f-4b16-9d79-6da19826bc84  Female\n",
       "ffc05de3-4806-484d-a7e5-c1931aba53f4    Male\n",
       "ffc1890c-8a22-44cf-927c-a7f203dac13c  Female\n",
       "ffd2313f-4be9-4ed1-b275-52a2d9a91fff    Male\n",
       "ffe6fcef-479a-4fab-b70c-07468b8f4c00    Male\n",
       "ffed63d3-f353-42a5-a290-06a102b03483    Male\n",
       "fff93ef4-c4d5-4812-a30c-211588f742d8  Female\n",
       "\n",
       "[5332 rows x 1 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at our label file\n",
    "label_df = pd.read_csv(label_table_file, delimiter='\\t')\n",
    "label_df = label_df.set_index('Subject')\n",
    "label_df\n",
    "#Access a label like this\n",
    "#print(label_df.loc[label_df.index[0], 'Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[ 1.26190186e-02, -5.95520020e-01, -7.89932251e-01,\n",
      "          9.89341319e-01],\n",
      "        [ 1.16173895e-02, -5.94906807e-01, -7.92927861e-01,\n",
      "          9.91402209e-01],\n",
      "        [ 1.06157586e-02, -5.94293654e-01, -7.95923531e-01,\n",
      "          9.93463099e-01],\n",
      "        ...,\n",
      "        [-3.44853811e-02, -6.38176262e-01, -7.72060752e-01,\n",
      "          1.00226545e+00],\n",
      "        [-3.44088264e-02, -6.38012052e-01, -7.71841228e-01,\n",
      "          1.00198913e+00],\n",
      "        [-3.43322754e-02, -6.37847900e-01, -7.71621704e-01,\n",
      "          1.00171292e+00]],\n",
      "\n",
      "       [[-3.43322754e-02, -6.37847900e-01, -7.71621704e-01,\n",
      "          1.00171292e+00],\n",
      "        [-3.33923809e-02, -6.36958838e-01, -7.74032891e-01,\n",
      "          1.00301957e+00],\n",
      "        [-3.24524902e-02, -6.36069775e-01, -7.76444077e-01,\n",
      "          1.00432622e+00],\n",
      "        ...,\n",
      "        [ 6.43885368e-03, -6.04356170e-01, -8.04146230e-01,\n",
      "          1.00596297e+00],\n",
      "        [ 5.95075078e-03, -6.05438888e-01, -8.04378748e-01,\n",
      "          1.00679135e+00],\n",
      "        [ 5.46264648e-03, -6.06521606e-01, -8.04611206e-01,\n",
      "          1.00761974e+00]],\n",
      "\n",
      "       [[ 5.46264648e-03, -6.06521606e-01, -8.04611206e-01,\n",
      "          1.00761974e+00],\n",
      "        [ 4.45496570e-03, -6.07449532e-01, -8.04706037e-01,\n",
      "          1.00825894e+00],\n",
      "        [ 3.44728376e-03, -6.08377457e-01, -8.04800868e-01,\n",
      "          1.00889814e+00],\n",
      "        ...,\n",
      "        [-2.20770089e-04, -6.04521990e-01, -8.03693950e-01,\n",
      "          1.00569260e+00],\n",
      "        [ 1.16372388e-03, -6.04186654e-01, -8.03008139e-01,\n",
      "          1.00493300e+00],\n",
      "        [ 2.54821777e-03, -6.03851318e-01, -8.02322388e-01,\n",
      "          1.00417340e+00]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 1.58538818e-02, -5.96817017e-01, -7.92633057e-01,\n",
      "          9.92325068e-01],\n",
      "        [ 1.64119899e-02, -5.96336663e-01, -7.94445574e-01,\n",
      "          9.93510067e-01],\n",
      "        [ 1.69700980e-02, -5.95856249e-01, -7.96258032e-01,\n",
      "          9.94695127e-01],\n",
      "        ...,\n",
      "        [-4.29886244e-02, -6.30441129e-01, -7.90670633e-01,\n",
      "          1.01216280e+00],\n",
      "        [-4.36042994e-02, -6.30352676e-01, -7.90743947e-01,\n",
      "          1.01218915e+00],\n",
      "        [-4.42199707e-02, -6.30264282e-01, -7.90817261e-01,\n",
      "          1.01221561e+00]],\n",
      "\n",
      "       [[-4.42199707e-02, -6.30264282e-01, -7.90817261e-01,\n",
      "          1.01221561e+00],\n",
      "        [-4.45968695e-02, -6.29474461e-01, -7.91002035e-01,\n",
      "          1.01189029e+00],\n",
      "        [-4.49737683e-02, -6.28684640e-01, -7.91186750e-01,\n",
      "          1.01156497e+00],\n",
      "        ...,\n",
      "        [ 8.83745681e-03, -5.69457352e-01, -7.57525384e-01,\n",
      "          9.47738767e-01],\n",
      "        [ 9.01162345e-03, -5.68992257e-01, -7.56341457e-01,\n",
      "          9.46513653e-01],\n",
      "        [ 9.18579102e-03, -5.68527222e-01, -7.55157471e-01,\n",
      "          9.45288539e-01]],\n",
      "\n",
      "       [[ 9.18579102e-03, -5.68527222e-01, -7.55157471e-01,\n",
      "          9.45288539e-01],\n",
      "        [ 9.09534283e-03, -5.72345436e-01, -7.63830304e-01,\n",
      "          9.54548001e-01],\n",
      "        [ 9.00489558e-03, -5.76163590e-01, -7.72503197e-01,\n",
      "          9.63807523e-01],\n",
      "        ...,\n",
      "        [-1.03621222e-02, -6.26521707e-01, -7.16845870e-01,\n",
      "          9.52430069e-01],\n",
      "        [-1.00867618e-02, -6.26981556e-01, -7.10138023e-01,\n",
      "          9.47541118e-01],\n",
      "        [-9.81140137e-03, -6.27441406e-01, -7.03430176e-01,\n",
      "          9.42652106e-01]]]), array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False]))\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Data generator\n",
    "# =============================================================================\n",
    "def extract_labels(labels, label_table_path):\n",
    "    '''\n",
    "    Returns a dataframe indexed by healthCodes with columns of requested labels\n",
    "    taken from the label table file\n",
    "    '''\n",
    "    label_df = pd.read_csv(label_table_file, delimiter='\\t')\n",
    "    return label_df.set_index('Subject')\n",
    "\n",
    "def parse_label(code, label_df):\n",
    "    \"\"\"\n",
    "    Helper function that parses the labels on survey data for a given code\n",
    "    \"\"\"\n",
    "    \n",
    "    return label_df.loc[label_df.index[0], 'Sex'] == \"Male\"\n",
    "        \n",
    "class SixMWTSequence(keras.utils.Sequence):\n",
    "    '''\n",
    "    SixMWTSequence\n",
    "    Extends keras inbuilt sequence to create a data generator\n",
    "    Saves on RAM by loading data from hdf5 files in memory\n",
    "    __del__ way of closing files isn't great - find a better way sometime\n",
    "    '''\n",
    "    def __init__(self, data_file, batch_size, label_df):\n",
    "        #Open up file\n",
    "        #self.lock = threading.Lock()\n",
    "        self.file = data_file\n",
    "        self.data = data_file.root.data\n",
    "        self.labels = data_file.root.labels\n",
    "        \n",
    "        #Track labels and batch size\n",
    "        self.label_map = label_df\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.num_data = self.labels.shape[0]\n",
    "        \n",
    "        self.inval_hc = set()\n",
    "\n",
    "        #Partition the dataset into batches\n",
    "        self.length = self.num_data // self.batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        #Find how many batches fit in our dataset\n",
    "        #This \"crops\" out a couple datapoints not divisible by the batch at the end\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        start_idx = idx*self.batch_size\n",
    "        stop_idx = (idx + 1)*self.batch_size\n",
    "            \n",
    "        #Get the batch members\n",
    "        batch_x = self.data[start_idx: stop_idx]\n",
    "        y_healthcodes = self.labels[start_idx: stop_idx]\n",
    "        \n",
    "        \n",
    "        #Convert healthcodes to genders\n",
    "        batch_y = np.empty(len(y_healthcodes), dtype = bool)\n",
    "        for i in range(len(batch_y)):\n",
    "            try:\n",
    "                batch_y[i] = parse_label(y_healthcodes[i], self.label_map)\n",
    "            except KeyError:\n",
    "                #Currently just assign random gender. This is a dumb idea.\n",
    "                self.inval_hc.add(y_healthcodes[i])\n",
    "                batch_y[i] = bool(random.randint(0,1))\n",
    "\n",
    "        return batch_x, batch_y\n",
    "\n",
    "label_df = extract_labels(labels_of_interest, label_table_file)\n",
    "#Look at what the data file looks like\n",
    "with tables.open_file(train_data_path, mode='r') as file:\n",
    "    train_gen = SixMWTSequence(file, batch_size, label_df)\n",
    "    print(train_gen[20])\n",
    "    x,y = train_gen[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Training the model\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "\n",
    "print(\"Loading filtered data\")\n",
    "with h5py.File(train_data_path, 'r') as filtered_train_file, h5py.File(val_data_path, 'r') as filtered_validation_file:\n",
    "    \n",
    "    training_batch_generator = SixMWTSequence(filtered_train_file, batch_size, label_df)\n",
    "    validation_batch_generator = SixMWTSequence(filtered_validation_file, batch_size, label_df)\n",
    "    \n",
    "    num_training_samples = len(training_batch_generator)\n",
    "    num_validation_samples = len(validation_batch_generator)\n",
    "    print(\"There are {} training samples and {} validation samples\".format(num_training_samples, num_validation_samples))\n",
    "    \n",
    "    num_epochs = 1000\n",
    "    \n",
    "    #Calculate class weights from 100 batches\n",
    "    temp = np.array([])\n",
    "    num_smpls = min(100, len(training_batch_generator))\n",
    "    rand_idxs = random.sample(list(range(len(training_batch_generator))), num_smpls)\n",
    "    for batch_num in rand_idxs:\n",
    "        _, temp_y = training_batch_generator[batch_num]\n",
    "        temp = np.concatenate((temp, temp_y))\n",
    "    class_weights = dict(enumerate(class_weight.compute_class_weight('balanced',\n",
    "                                                     np.unique(temp),\n",
    "                                                     temp)))\n",
    "    \n",
    "    print(\"Our class weights are {}\".format(class_weights))\n",
    "    \n",
    "    #Callbacks\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                  patience=5, min_lr=1e-7)\n",
    "    \n",
    "    early_stop = EarlyStopping(patience=7)\n",
    "    \n",
    "    tb = TensorBoard(log_dir=os.path.join(output_dir, 'logs'))\n",
    "    \n",
    "    history = model.fit_generator(generator=training_batch_generator,\n",
    "                                  epochs=num_epochs,\n",
    "                                  verbose=2,\n",
    "                                  callbacks = [reduce_lr, early_stop, tb],\n",
    "                                  validation_data=validation_batch_generator,\n",
    "                                  class_weight=class_weights,\n",
    "                                  use_multiprocessing=canMultiprocess, \n",
    "                                  workers=4,\n",
    "                                  max_queue_size=32,\n",
    "                                  shuffle=True)\n",
    "    \n",
    "    print(\"Finished training, beginning cleanup.\")\n",
    "    #Clean up the temp files\n",
    "    del training_batch_generator\n",
    "    del validation_batch_generator\n",
    "\n",
    "#Save history and model\n",
    "with open(os.path.join(output_dir, 'train_history.pkl'), 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "model.save(os.path.join(output_dir, \"model.h5\"))\n",
    "\n",
    "print(\"All done, results saved in {}\".format(output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a model\n",
    "def get_model():\n",
    "    model = Sequential()\n",
    "    # ENTRY LAYER\n",
    "    model.add(Conv1D(100, 20, activation='relu', input_shape=(200, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv1D(100, 20, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(3))\n",
    "\n",
    "    model.add(Conv1D(160, 20, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv1D(160, 20, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(40, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the model up\n",
    "model = get_model()\n",
    "print(model.summary())\n",
    "\n",
    "#Loss function - taken from kerasAC.custom_losses  -   need to figure out weights before using\n",
    "#def get_weighted_binary_crossentropy(w0_weights, w1_weights):\n",
    "#    import keras.backend as K\n",
    "#    # Compute the task-weighted cross-entropy loss, where every task is weighted by 1 - (fraction of non-ambiguous examples that are positive)\n",
    "#    # In addition, weight everything with label -1 to 0\n",
    "#    w0_weights=np.array(w0_weights);\n",
    "#    w1_weights=np.array(w1_weights);\n",
    "#    thresh=-0.5\n",
    "#\n",
    "#    def weighted_binary_crossentropy(y_true,y_pred):\n",
    "#        weightsPerTaskRep = y_true*w1_weights[None,:] + (1-y_true)*w0_weights[None,:]\n",
    "#        nonAmbig = K.cast((y_true > -0.5),'float32')\n",
    "#        nonAmbigTimesWeightsPerTask = nonAmbig * weightsPerTaskRep\n",
    "#        return K.mean(K.binary_crossentropy(y_pred, y_true)*nonAmbigTimesWeightsPerTask, axis=-1);\n",
    "#    return weighted_binary_crossentropy; \n",
    "\n",
    "\n",
    "\n",
    "#Define optimizer\n",
    "adam = keras.optimizers.Adam(lr=0.01) #Default is 0.001\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=model_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing healthcode 18bcdec4-2903-4f36-b7bc-8a1fa95b6a7a\n",
      "Processing healthcode a4898c78-d896-44ba-a093-37d9bc52446f\n",
      "Processing healthcode 94150c76-17ff-4000-8f0f-8f3297edd026\n",
      "Processing healthcode c90e2feb-9856-47b7-891b-6edcd5610f99\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Jun 24 18:08:32 2019\n",
    "\n",
    "Parses raw walk cycles into normalized cycles with LSE, and saves to numpy array\n",
    "\n",
    "See the Data Preprocessing Notebook for more information.\n",
    "\n",
    "@author: dwubu\n",
    "\"\"\"\n",
    "\n",
    "#Imports\n",
    "import lomb_scargle_extractor as lse\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tables\n",
    "\n",
    "out_path = r\"/scratch/PI/euan/projects/mhc/code/daniel_code\"\n",
    "data_dir = r\"/scratch/PI/euan/projects/mhc/data/6mwt/accel_walk_dir\"\n",
    " \n",
    "#out_path = r'C:\\Users\\dwubu\\Desktop'\n",
    "#data_dir = r'C:\\Users\\dwubu\\Desktop\\accel_walk_dir'\n",
    "   \n",
    "def ls_extract(data):\n",
    "    '''\n",
    "    Takes in a 6mwt in MHC format: list of dict objects, with each dict\n",
    "    containing a single time point with \"timestamp\", \"x\", \"y\", \"z\" keys\n",
    "    does Lomb-Scargle extraction.\n",
    "    \n",
    "    Returns a (n x 100 x 4) numpy array\n",
    "    of n walk cycles interpolated and aligned to 100 datapoints with 4 channels:\n",
    "    x, y, z, mag\n",
    "    '''\n",
    "\n",
    "    #Convert data from list of json to a pandas dataframe\n",
    "    df = pd.DataFrame(columns=['timestamp','x','y','z'])\n",
    "    for row in data:\n",
    "        df = df.append(pd.Series(row), ignore_index=True)\n",
    "        \n",
    "    #Conform to lse api - reindex, add mag column, and rename\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "    temp_idx = pd.DatetimeIndex(df['timestamp'])\n",
    "    df = df.drop('timestamp', axis = 1)\n",
    "    norm = np.sqrt(np.square(df).sum(axis=1))\n",
    "    df = pd.concat([df, norm], axis=1)\n",
    "    df = df.rename(index=str, columns={'x': 'a_x', 'y': 'a_y', 'z': 'a_z', 0: 'a_mag'})\n",
    "    df = df.set_index(temp_idx)\n",
    "    \n",
    "    #Chunk data, and extract windows from each chunk\n",
    "    chunk_size = 500\n",
    "    cycles = np.empty((0, 100, 4))\n",
    "    for idx in range(0, df.shape[0], chunk_size):\n",
    "    \n",
    "        chunk = df[idx: idx+chunk_size]\n",
    "        \n",
    "        try:\n",
    "            test_cycle = lse.extract(chunk)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        if(isinstance(test_cycle, pd.DataFrame)):\n",
    "            #Reshape output and concatenate into the n x 100 x 4 format\n",
    "            mag_cycle = np.reshape(list(test_cycle['a_mag']), (100, -1), order='F').T\n",
    "            x_cycle = np.reshape(list(test_cycle['a_x']), (100, -1), order='F').T\n",
    "            y_cycle = np.reshape(list(test_cycle['a_y']), (100, -1), order='F').T\n",
    "            z_cycle = np.reshape(list(test_cycle['a_z']), (100, -1), order='F').T\n",
    "            all_cycle = np.stack([x_cycle, y_cycle, z_cycle, mag_cycle], -1)\n",
    "    \n",
    "            #Store our current cycle into a container\n",
    "            cycles = np.vstack((cycles, all_cycle))\n",
    "                    \n",
    "        \n",
    "    return cycles\n",
    "\n",
    "    \n",
    "#Initialize the file\n",
    "with tables.open_file(os.path.join(out_path, \"cycles.hdf5\"), mode='w', title = \"cycles\") as file:\n",
    "    earray = file.create_earray(file.root, \"data\", \n",
    "                                atom = tables.Float64Atom(), \n",
    "                                shape=(0, 100, 4), \n",
    "                                expectedrows = 5e7)\n",
    "    labels = file.create_earray(file.root, \"labels\", \n",
    "                                atom = tables.StringAtom(itemsize=40), \n",
    "                                shape=(0,), \n",
    "                                expectedrows = 5e7)\n",
    "    #8126 healthcodes * 1 6mwt/healthcode * 600 steps/6mwt ~ 5e7\n",
    "\n",
    "    #Iterate through all the records in the directory\n",
    "    for dirpath, dirnames, filenames in os.walk(data_dir):\n",
    "        i = 0\n",
    "        for filename in filenames:\n",
    "\n",
    "            #Only take one 6mwt per healthcode        \n",
    "            while (i < 1):\n",
    "                i += 1\n",
    "\n",
    "                healthCode = dirpath.split(os.sep)[-1]\n",
    "                print(\"Processing healthcode {}\".format(healthCode))\n",
    "\n",
    "                #Load in data and get ready for LSE\n",
    "                with open(os.path.join(dirpath, filename), 'r') as file:\n",
    "                    data = json.load(file)\n",
    "\n",
    "                #Extract\n",
    "                temp_cycles = ls_extract(data)\n",
    "\n",
    "                #Store in pytables\n",
    "                #with tables.open_file(os.path.join(out_path, \"cycles.hdf5\"), mode='a', title = \"cycles\") as h5_file:\n",
    "                #    h5_file.root.data.append(temp_cycles)\n",
    "                earray.append(temp_cycles)\n",
    "                labels.append([healthCode]*temp_cycles.shape[0])\n",
    "\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
