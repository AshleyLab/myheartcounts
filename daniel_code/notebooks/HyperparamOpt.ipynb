{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning notebook with Hyperas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape, Input, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(42)\n",
    "\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    " \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[0], batch_size = batch_size))).round()\n",
    "        val_targ = self.validation_data[1]\n",
    "        \n",
    "        _val_f1 = f1_score(val_targ, val_predict)\n",
    "        _val_recall = recall_score(val_targ, val_predict)\n",
    "        _val_precision = precision_score(val_targ, val_predict)\n",
    "        \n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        print(f' — val_f1: {_val_f1} — val_precision: {_val_precision} — val_recall {_val_recall}')\n",
    "        return\n",
    " \n",
    "metrix = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    \"\"\"\n",
    "    Data providing function:\n",
    "\n",
    "    This function is separated from create_model() so that hyperopt\n",
    "    won't reload data for each evaluation run.\n",
    "    \"\"\"\n",
    "    print('Beginning loading')\n",
    "\n",
    "    #X_train = np.load('processed_datasets/Sex_X_15_xtremefiltered.npy')\n",
    "    # y_train = np.load('processed_datasets/Sex_y_15_xtremefiltered.npy')\n",
    "    #ids_train = np.load('processed_datasets/Sex_ids_15_xtremefiltered.npy')\n",
    "    \n",
    "    X_train = np.load('processed_datasets/Sex_X_15_aligned.npy')\n",
    "    y_train = np.load('processed_datasets/Sex_y_15_aligned.npy')\n",
    "    ids_train = np.load('processed_datasets/Sex_ids_15_aligned.npy')\n",
    "\n",
    "    print('Finished loading, beginning X_train axis adjustment')\n",
    "\n",
    "    #Change X_train to weird format\n",
    "    X_train = np.expand_dims(X_train, 1)\n",
    "    X_train = np.swapaxes(X_train, -1, -2)\n",
    "\n",
    "    print('Beginning y_train encoding')\n",
    "\n",
    "    #Convert string to binary labels\n",
    "    y_train = y_train == 'Male'\n",
    "\n",
    "    #Randomize\n",
    "    if False:\n",
    "        print('Data will be randomized.')\n",
    "        idxs = np.arange(X_train.shape[0])\n",
    "        np.random.shuffle(idxs)\n",
    "        X_train = X_train[idxs]\n",
    "        y_train = y_train[idxs]\n",
    "        ids_train = ids_train[idxs]\n",
    "    else:\n",
    "        print('Data is NOT randomized.')\n",
    "\n",
    "    print('Beginning data splitting')\n",
    "\n",
    "    split_num = int(0.8*X_train.shape[0])\n",
    "    X_test = X_train[split_num:]\n",
    "    y_test = y_train[split_num:]\n",
    "    ids_test = ids_train[split_num:]\n",
    "\n",
    "    X_train = X_train[:split_num]\n",
    "    y_train = y_train[:split_num]\n",
    "    ids_train = ids_train[:split_num]\n",
    "    \n",
    "    print(X_train.shape, X_test.shape, y_train.shape, y_test.shape, ids_train.shape, ids_test.shape)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Model providing function:\n",
    "\n",
    "    Create Keras model with double curly brackets dropped-in as needed.\n",
    "    Return value has to be a valid python dictionary with two customary keys:\n",
    "        - loss: Specify a numeric evaluation metric to be minimized\n",
    "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "    The last one is optional, though recommended, namely:\n",
    "        - model: specify the model just created so that we can later use it again.\n",
    "    \"\"\"\n",
    "    \n",
    "    #VGG\n",
    "    import functools\n",
    "    import json\n",
    "\n",
    "    import keras.backend as K\n",
    "\n",
    "    from keras.layers import Permute, Dense, Input, Conv2D, concatenate, MaxPooling2D\n",
    "    from keras.layers import ELU, BatchNormalization, Dropout, GlobalAveragePooling2D\n",
    "    from keras.models import Model\n",
    "\n",
    "    # %load model_util\n",
    "    from keras.layers import Input\n",
    "    from keras.regularizers import l2\n",
    "\n",
    "\n",
    "    REG_P = dict(kernel_regularizer=l2({{choice([0.001, 0.01, 0.05])}}))\n",
    "\n",
    "\n",
    "    def _create_input(input_shape, input_tensor=None, name='input'):\n",
    "        \"\"\"\n",
    "        Select a correct input tensor based on shape and instance specification.\n",
    "\n",
    "        # Arguments\n",
    "            input_shape: Input shape tuple\n",
    "            input_tensor: Existing tensor to wrap into the `Input` layer.\n",
    "                          If set, the layer will not create a placeholder tensor.\n",
    "            name: Name string for layer.\n",
    "\n",
    "        # Returns\n",
    "            Input Tensor\n",
    "        \"\"\"\n",
    "        if input_tensor is None:\n",
    "            return Input(shape=input_shape, name=name)\n",
    "\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            return Input(tensor=input_tensor, shape=input_shape, name=name)\n",
    "\n",
    "        return input_tensor\n",
    "\n",
    "\n",
    "    def _conv_block(units, block=1, layer=1, sensor='acc'):\n",
    "        \"\"\"\n",
    "        Create VGG style convolutional block.\n",
    "\n",
    "        Deviations from original paper.\n",
    "            - Remove `Dropout`\n",
    "            - Added `BatchNormalization`\n",
    "            - He-normal initialization\n",
    "            - Uses `ELU` Activation\n",
    "\n",
    "        # Arguments\n",
    "            units: conv filters\n",
    "            block: block number within network (used for naming)\n",
    "            layer: layer number within block (used for naming)\n",
    "            sensor: sensor name (used for naming)\n",
    "\n",
    "        # Returns\n",
    "            callable\n",
    "        \"\"\"\n",
    "        def layer_wrapper(inp):\n",
    "            filter_len = {{choice([3, 5])}}\n",
    "            x = Conv2D(units, (1, filter_len), padding='same', kernel_initializer='he_normal',\n",
    "                       name=f'block{block}_conv{layer}_{sensor}', **REG_P)(inp)\n",
    "            x = BatchNormalization(name=f'block{block}_bn{layer}_{sensor}')(x)\n",
    "            x = ELU(name=f'block{block}_act{layer}_{sensor}')(x)\n",
    "            return x\n",
    "\n",
    "        return layer_wrapper\n",
    "\n",
    "\n",
    "    def _dense_block(units, dropout=0.3, name='fc1'):\n",
    "        \"\"\"\n",
    "        Create VGG fully connected block.\n",
    "\n",
    "        # Deviations from original paper.\n",
    "            - Added `BatchNormalization`\n",
    "            - Uses `ELU` Activation\n",
    "\n",
    "        # Arguments\n",
    "            units: fc layer dimensionality\n",
    "            dropout: dropout probability\n",
    "            name: prefix for dense layers\n",
    "\n",
    "        # Returns\n",
    "            callable\n",
    "        \"\"\"\n",
    "\n",
    "        def layer_wrapper(inp):\n",
    "            x = Dense(units, name=f'{name}', **REG_P)(inp)\n",
    "            x = BatchNormalization(name=f'{name}_bn')(x)\n",
    "            x = ELU(name=f'{name}_act')(x)\n",
    "            x = Dropout(dropout, name=f'{name}_dropout')(x)\n",
    "            return x\n",
    "\n",
    "        return layer_wrapper\n",
    "\n",
    "\n",
    "    def _vgg_body(factor=4, sensor='acc'):\n",
    "        \"\"\"\n",
    "        VGG Network Body containing convolutional blocks\n",
    "\n",
    "        # Arguments\n",
    "            factor: scaling factor to reduce network filter width\n",
    "            sensor: sensor name\n",
    "\n",
    "        # Return\n",
    "            callable\n",
    "        \"\"\"\n",
    "\n",
    "        _vgg_conv_block = functools.partial(_conv_block, sensor=sensor)\n",
    "\n",
    "        def layer_wrapper(inp):\n",
    "            x = Permute((1, 3, 2), name=f'swapaxes_{sensor}')(inp)\n",
    "            \n",
    "            num_blocks = {{choice([2, 3, 4, 5])}}\n",
    "            \n",
    "            for block_num in range(1, num_blocks + 1):\n",
    "\n",
    "                x = _vgg_conv_block(32 * (2**block_num) // factor, block=block_num, layer=1)(x)\n",
    "                x = _vgg_conv_block(32 * (2**block_num) // factor, block=block_num, layer=2)(x)\n",
    "                x = MaxPooling2D((1, {{choice([2, 4])}}), name=f'block{block_num}_pool_{sensor}')(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "        return layer_wrapper\n",
    "\n",
    "    def VGG16Net(input_shape=None, input_tensor=(None, None),\n",
    "                 classes=1000, dropout=0.3, factor=2):\n",
    "        \"\"\"\n",
    "        Modified VGG architecture\n",
    "            https://arxiv.org/abs/1409.1556\n",
    "\n",
    "        # Arguments\n",
    "            input_shape: shape tuple\n",
    "            input_tensor: Keras tensor (i.e. output of `layers.Input()`) to use as image input for the model.\n",
    "            classes: optional number of classes to classify images\n",
    "            dropout: dropout applied to fc layers\n",
    "            factor: scaling factor to reduce network filter width\n",
    "\n",
    "        # Returns\n",
    "            A Keras model instance.\n",
    "        \"\"\"\n",
    "        assert input_shape or all(input_tensor), f'Must provide at least one: input_shape, input_tensor'\n",
    "\n",
    "        # Two Inputs\n",
    "        img_input_acc = _create_input(input_shape, input_tensor=input_tensor[0], name='acc_input')\n",
    "\n",
    "        # Accelerometer and Gyroscope Conv Blocks\n",
    "        x = _vgg_body(factor=factor, sensor='acc')(img_input_acc)\n",
    "\n",
    "        # Merge and Pool Channels\n",
    "        x = GlobalAveragePooling2D(name='avgpool')(x)\n",
    "\n",
    "        # FC Layers\n",
    "        num_dense = {{choice([1, 2])}}\n",
    "        for i in range(num_dense):\n",
    "            x = _dense_block(4098 // factor, dropout=dropout, name=f'fc{i + 1}')(x)\n",
    "\n",
    "        # Classification block\n",
    "        if classes == 2:\n",
    "            x = Dense(1, activation = 'sigmoid', name='predictions')(x)\n",
    "        else:\n",
    "            x = Dense(classes, activation='softmax', name='predictions')(x)\n",
    "\n",
    "        return Model(img_input_acc, x, name='VGG16Net')\n",
    "    \n",
    "    model = VGG16Net(input_shape=(1, 15, 100), classes=2)\n",
    "\n",
    "    #Define optimizer\n",
    "    adam = keras.optimizers.Adam() #Default lr is 0.001\n",
    "\n",
    "    model_metrics = ['accuracy']\n",
    "\n",
    "    #Training parameters\n",
    "    batch_size = 512\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=model_metrics)\n",
    "\n",
    "    #Callbacks\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                  patience=2, min_lr=1e-7, verbose=1)\n",
    "\n",
    "    early_stop = EarlyStopping(patience=4, verbose=1, restore_best_weights=True)\n",
    "\n",
    "    #tb = TensorBoard(log_dir=os.path.join(output_dir, 'logs'), \n",
    "    #                 write_graph=False,) \n",
    "                     #histogram_freq=5, \n",
    "                     #embeddings_freq=5, \n",
    "                     #embeddings_layer_names=['fc1'],\n",
    "                     #mbeddings_data = X_val)\n",
    "\n",
    "    cb_list = [reduce_lr, early_stop]\n",
    "\n",
    "    #Get data and train\n",
    "    #Use a generator for smaller epochs\n",
    "    class DataGenerator(keras.utils.Sequence):\n",
    "        'Generates data for Keras'\n",
    "        def __init__(self, X, y, batch_size=512, shuffle=True):\n",
    "            self.batch_size = batch_size\n",
    "            self.y = y\n",
    "            self.X = X\n",
    "            self.shuffle = shuffle\n",
    "            self.on_epoch_end()\n",
    "\n",
    "        def __len__(self):\n",
    "            'Denotes the number of batches per epoch'\n",
    "            return int(np.floor(len(self.y) / self.batch_size))\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            'Generate one batch of data'\n",
    "            idxs = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "            # Generate the batch\n",
    "            X = self.X[idxs]\n",
    "            y = self.y[idxs]\n",
    "\n",
    "            return X, y\n",
    "\n",
    "        def on_epoch_end(self):\n",
    "            'Updates indexes after each epoch'\n",
    "            self.indexes = np.arange(len(self.X))\n",
    "            if self.shuffle == True:\n",
    "                np.random.shuffle(self.indexes)\n",
    "    train_gen = DataGenerator(X_train, y_train)\n",
    "\n",
    "    history = model.fit_generator(train_gen,\n",
    "                        steps_per_epoch = 200,\n",
    "                        epochs = 200,\n",
    "                        validation_data = (X_test, y_test),#.squeeze().swapaxes(-1, -2), y_val),\n",
    "                        callbacks = cb_list,\n",
    "                        class_weight={0: 1.5, 1: 0.5}, #class_weights,\n",
    "                        shuffle=True,\n",
    "                        verbose=0)\n",
    "    \n",
    "    #get the highest validation accuracy of the training epochs\n",
    "    validation_acc = np.amax(history.history['val_acc']) \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense, Dropout, Flatten, Reshape, Input, BatchNormalization\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from matplotlib import pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.utils import class_weight\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import Callback\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import functools\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import json\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras.backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Permute, Dense, Input, Conv2D, concatenate, MaxPooling2D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import ELU, BatchNormalization, Dropout, GlobalAveragePooling2D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Input\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.regularizers import l2\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'l2': hp.choice('l2', [0.001, 0.01, 0.05]),\n",
      "        'filter_len': hp.choice('filter_len', [3, 5]),\n",
      "        'num_blocks': hp.choice('num_blocks', [2, 3, 4, 5]),\n",
      "        'num_blocks_1': hp.choice('num_blocks_1', [2, 4]),\n",
      "        'num_dense': hp.choice('num_dense', [1, 2]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: \"\"\"\n",
      "   3: Data providing function:\n",
      "   4: \n",
      "   5: This function is separated from create_model() so that hyperopt\n",
      "   6: won't reload data for each evaluation run.\n",
      "   7: \"\"\"\n",
      "   8: print('Beginning loading')\n",
      "   9: \n",
      "  10: #X_train = np.load('processed_datasets/Sex_X_15_xtremefiltered.npy')\n",
      "  11: # y_train = np.load('processed_datasets/Sex_y_15_xtremefiltered.npy')\n",
      "  12: #ids_train = np.load('processed_datasets/Sex_ids_15_xtremefiltered.npy')\n",
      "  13: \n",
      "  14: X_train = np.load('processed_datasets/Sex_X_15_aligned.npy')\n",
      "  15: y_train = np.load('processed_datasets/Sex_y_15_aligned.npy')\n",
      "  16: ids_train = np.load('processed_datasets/Sex_ids_15_aligned.npy')\n",
      "  17: \n",
      "  18: print('Finished loading, beginning X_train axis adjustment')\n",
      "  19: \n",
      "  20: #Change X_train to weird format\n",
      "  21: X_train = np.expand_dims(X_train, 1)\n",
      "  22: X_train = np.swapaxes(X_train, -1, -2)\n",
      "  23: \n",
      "  24: print('Beginning y_train encoding')\n",
      "  25: \n",
      "  26: #Convert string to binary labels\n",
      "  27: y_train = y_train == 'Male'\n",
      "  28: \n",
      "  29: #Randomize\n",
      "  30: if False:\n",
      "  31:     print('Data will be randomized.')\n",
      "  32:     idxs = np.arange(X_train.shape[0])\n",
      "  33:     np.random.shuffle(idxs)\n",
      "  34:     X_train = X_train[idxs]\n",
      "  35:     y_train = y_train[idxs]\n",
      "  36:     ids_train = ids_train[idxs]\n",
      "  37: else:\n",
      "  38:     print('Data is NOT randomized.')\n",
      "  39: \n",
      "  40: print('Beginning data splitting')\n",
      "  41: \n",
      "  42: split_num = int(0.8*X_train.shape[0])\n",
      "  43: X_test = X_train[split_num:]\n",
      "  44: y_test = y_train[split_num:]\n",
      "  45: ids_test = ids_train[split_num:]\n",
      "  46: \n",
      "  47: X_train = X_train[:split_num]\n",
      "  48: y_train = y_train[:split_num]\n",
      "  49: ids_train = ids_train[:split_num]\n",
      "  50: \n",
      "  51: print(X_train.shape, X_test.shape, y_train.shape, y_test.shape, ids_train.shape, ids_test.shape)\n",
      "  52: \n",
      "  53: \n",
      "  54: \n",
      "  55: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \"\"\"\n",
      "   4:     Model providing function:\n",
      "   5: \n",
      "   6:     Create Keras model with double curly brackets dropped-in as needed.\n",
      "   7:     Return value has to be a valid python dictionary with two customary keys:\n",
      "   8:         - loss: Specify a numeric evaluation metric to be minimized\n",
      "   9:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
      "  10:     The last one is optional, though recommended, namely:\n",
      "  11:         - model: specify the model just created so that we can later use it again.\n",
      "  12:     \"\"\"\n",
      "  13:     \n",
      "  14:     #VGG\n",
      "  15: \n",
      "  16: \n",
      "  17: \n",
      "  18:     # %load model_util\n",
      "  19: \n",
      "  20: \n",
      "  21:     REG_P = dict(kernel_regularizer=l2(space['l2']))\n",
      "  22: \n",
      "  23: \n",
      "  24:     def _create_input(input_shape, input_tensor=None, name='input'):\n",
      "  25:         \"\"\"\n",
      "  26:         Select a correct input tensor based on shape and instance specification.\n",
      "  27: \n",
      "  28:         # Arguments\n",
      "  29:             input_shape: Input shape tuple\n",
      "  30:             input_tensor: Existing tensor to wrap into the `Input` layer.\n",
      "  31:                           If set, the layer will not create a placeholder tensor.\n",
      "  32:             name: Name string for layer.\n",
      "  33: \n",
      "  34:         # Returns\n",
      "  35:             Input Tensor\n",
      "  36:         \"\"\"\n",
      "  37:         if input_tensor is None:\n",
      "  38:             return Input(shape=input_shape, name=name)\n",
      "  39: \n",
      "  40:         if not K.is_keras_tensor(input_tensor):\n",
      "  41:             return Input(tensor=input_tensor, shape=input_shape, name=name)\n",
      "  42: \n",
      "  43:         return input_tensor\n",
      "  44: \n",
      "  45: \n",
      "  46:     def _conv_block(units, block=1, layer=1, sensor='acc'):\n",
      "  47:         \"\"\"\n",
      "  48:         Create VGG style convolutional block.\n",
      "  49: \n",
      "  50:         Deviations from original paper.\n",
      "  51:             - Remove `Dropout`\n",
      "  52:             - Added `BatchNormalization`\n",
      "  53:             - He-normal initialization\n",
      "  54:             - Uses `ELU` Activation\n",
      "  55: \n",
      "  56:         # Arguments\n",
      "  57:             units: conv filters\n",
      "  58:             block: block number within network (used for naming)\n",
      "  59:             layer: layer number within block (used for naming)\n",
      "  60:             sensor: sensor name (used for naming)\n",
      "  61: \n",
      "  62:         # Returns\n",
      "  63:             callable\n",
      "  64:         \"\"\"\n",
      "  65:         def layer_wrapper(inp):\n",
      "  66:             filter_len = space['filter_len']\n",
      "  67:             x = Conv2D(units, (1, filter_len), padding='same', kernel_initializer='he_normal',\n",
      "  68:                        name=f'block{block}_conv{layer}_{sensor}', **REG_P)(inp)\n",
      "  69:             x = BatchNormalization(name=f'block{block}_bn{layer}_{sensor}')(x)\n",
      "  70:             x = ELU(name=f'block{block}_act{layer}_{sensor}')(x)\n",
      "  71:             return x\n",
      "  72: \n",
      "  73:         return layer_wrapper\n",
      "  74: \n",
      "  75: \n",
      "  76:     def _dense_block(units, dropout=0.3, name='fc1'):\n",
      "  77:         \"\"\"\n",
      "  78:         Create VGG fully connected block.\n",
      "  79: \n",
      "  80:         # Deviations from original paper.\n",
      "  81:             - Added `BatchNormalization`\n",
      "  82:             - Uses `ELU` Activation\n",
      "  83: \n",
      "  84:         # Arguments\n",
      "  85:             units: fc layer dimensionality\n",
      "  86:             dropout: dropout probability\n",
      "  87:             name: prefix for dense layers\n",
      "  88: \n",
      "  89:         # Returns\n",
      "  90:             callable\n",
      "  91:         \"\"\"\n",
      "  92: \n",
      "  93:         def layer_wrapper(inp):\n",
      "  94:             x = Dense(units, name=f'{name}', **REG_P)(inp)\n",
      "  95:             x = BatchNormalization(name=f'{name}_bn')(x)\n",
      "  96:             x = ELU(name=f'{name}_act')(x)\n",
      "  97:             x = Dropout(dropout, name=f'{name}_dropout')(x)\n",
      "  98:             return x\n",
      "  99: \n",
      " 100:         return layer_wrapper\n",
      " 101: \n",
      " 102: \n",
      " 103:     def _vgg_body(factor=4, sensor='acc'):\n",
      " 104:         \"\"\"\n",
      " 105:         VGG Network Body containing convolutional blocks\n",
      " 106: \n",
      " 107:         # Arguments\n",
      " 108:             factor: scaling factor to reduce network filter width\n",
      " 109:             sensor: sensor name\n",
      " 110: \n",
      " 111:         # Return\n",
      " 112:             callable\n",
      " 113:         \"\"\"\n",
      " 114: \n",
      " 115:         _vgg_conv_block = functools.partial(_conv_block, sensor=sensor)\n",
      " 116: \n",
      " 117:         def layer_wrapper(inp):\n",
      " 118:             x = Permute((1, 3, 2), name=f'swapaxes_{sensor}')(inp)\n",
      " 119:             \n",
      " 120:             num_blocks = space['num_blocks']\n",
      " 121:             \n",
      " 122:             for block_num in range(1, num_blocks + 1):\n",
      " 123: \n",
      " 124:                 x = _vgg_conv_block(32 * (2**block_num) // factor, block=block_num, layer=1)(x)\n",
      " 125:                 x = _vgg_conv_block(32 * (2**block_num) // factor, block=block_num, layer=2)(x)\n",
      " 126:                 x = MaxPooling2D((1, space['num_blocks_1']), name=f'block{block_num}_pool_{sensor}')(x)\n",
      " 127: \n",
      " 128:             return x\n",
      " 129: \n",
      " 130:         return layer_wrapper\n",
      " 131: \n",
      " 132:     def VGG16Net(input_shape=None, input_tensor=(None, None),\n",
      " 133:                  classes=1000, dropout=0.3, factor=2):\n",
      " 134:         \"\"\"\n",
      " 135:         Modified VGG architecture\n",
      " 136:             https://arxiv.org/abs/1409.1556\n",
      " 137: \n",
      " 138:         # Arguments\n",
      " 139:             input_shape: shape tuple\n",
      " 140:             input_tensor: Keras tensor (i.e. output of `layers.Input()`) to use as image input for the model.\n",
      " 141:             classes: optional number of classes to classify images\n",
      " 142:             dropout: dropout applied to fc layers\n",
      " 143:             factor: scaling factor to reduce network filter width\n",
      " 144: \n",
      " 145:         # Returns\n",
      " 146:             A Keras model instance.\n",
      " 147:         \"\"\"\n",
      " 148:         assert input_shape or all(input_tensor), f'Must provide at least one: input_shape, input_tensor'\n",
      " 149: \n",
      " 150:         # Two Inputs\n",
      " 151:         img_input_acc = _create_input(input_shape, input_tensor=input_tensor[0], name='acc_input')\n",
      " 152: \n",
      " 153:         # Accelerometer and Gyroscope Conv Blocks\n",
      " 154:         x = _vgg_body(factor=factor, sensor='acc')(img_input_acc)\n",
      " 155: \n",
      " 156:         # Merge and Pool Channels\n",
      " 157:         x = GlobalAveragePooling2D(name='avgpool')(x)\n",
      " 158: \n",
      " 159:         # FC Layers\n",
      " 160:         num_dense = space['num_dense']\n",
      " 161:         for i in range(num_dense):\n",
      " 162:             x = _dense_block(4098 // factor, dropout=dropout, name=f'fc{i + 1}')(x)\n",
      " 163: \n",
      " 164:         # Classification block\n",
      " 165:         if classes == 2:\n",
      " 166:             x = Dense(1, activation = 'sigmoid', name='predictions')(x)\n",
      " 167:         else:\n",
      " 168:             x = Dense(classes, activation='softmax', name='predictions')(x)\n",
      " 169: \n",
      " 170:         return Model(img_input_acc, x, name='VGG16Net')\n",
      " 171:     \n",
      " 172:     model = VGG16Net(input_shape=(1, 15, 100), classes=2)\n",
      " 173: \n",
      " 174:     #Define optimizer\n",
      " 175:     adam = keras.optimizers.Adam() #Default lr is 0.001\n",
      " 176: \n",
      " 177:     model_metrics = ['accuracy']\n",
      " 178: \n",
      " 179:     #Training parameters\n",
      " 180:     batch_size = 512\n",
      " 181:     \n",
      " 182:     model.compile(loss='binary_crossentropy',\n",
      " 183:                   optimizer=adam,\n",
      " 184:                   metrics=model_metrics)\n",
      " 185: \n",
      " 186:     #Callbacks\n",
      " 187:     reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
      " 188:                                   patience=2, min_lr=1e-7, verbose=1)\n",
      " 189: \n",
      " 190:     early_stop = EarlyStopping(patience=4, verbose=1, restore_best_weights=True)\n",
      " 191: \n",
      " 192:     #tb = TensorBoard(log_dir=os.path.join(output_dir, 'logs'), \n",
      " 193:     #                 write_graph=False,) \n",
      " 194:                      #histogram_freq=5, \n",
      " 195:                      #embeddings_freq=5, \n",
      " 196:                      #embeddings_layer_names=['fc1'],\n",
      " 197:                      #mbeddings_data = X_val)\n",
      " 198: \n",
      " 199:     cb_list = [reduce_lr, early_stop]\n",
      " 200: \n",
      " 201:     #Get data and train\n",
      " 202:     #Use a generator for smaller epochs\n",
      " 203:     class DataGenerator(keras.utils.Sequence):\n",
      " 204:         'Generates data for Keras'\n",
      " 205:         def __init__(self, X, y, batch_size=512, shuffle=True):\n",
      " 206:             self.batch_size = batch_size\n",
      " 207:             self.y = y\n",
      " 208:             self.X = X\n",
      " 209:             self.shuffle = shuffle\n",
      " 210:             self.on_epoch_end()\n",
      " 211: \n",
      " 212:         def __len__(self):\n",
      " 213:             'Denotes the number of batches per epoch'\n",
      " 214:             return int(np.floor(len(self.y) / self.batch_size))\n",
      " 215: \n",
      " 216:         def __getitem__(self, index):\n",
      " 217:             'Generate one batch of data'\n",
      " 218:             idxs = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
      " 219: \n",
      " 220:             # Generate the batch\n",
      " 221:             X = self.X[idxs]\n",
      " 222:             y = self.y[idxs]\n",
      " 223: \n",
      " 224:             return X, y\n",
      " 225: \n",
      " 226:         def on_epoch_end(self):\n",
      " 227:             'Updates indexes after each epoch'\n",
      " 228:             self.indexes = np.arange(len(self.X))\n",
      " 229:             if self.shuffle == True:\n",
      " 230:                 np.random.shuffle(self.indexes)\n",
      " 231:     train_gen = DataGenerator(X_train, y_train)\n",
      " 232: \n",
      " 233:     history = model.fit_generator(train_gen,\n",
      " 234:                         steps_per_epoch = 200,\n",
      " 235:                         epochs = 200,\n",
      " 236:                         validation_data = (X_test, y_test),#.squeeze().swapaxes(-1, -2), y_val),\n",
      " 237:                         callbacks = cb_list,\n",
      " 238:                         class_weight={0: 1.5, 1: 0.5}, #class_weights,\n",
      " 239:                         shuffle=True,\n",
      " 240:                         verbose=0)\n",
      " 241:     \n",
      " 242:     #get the highest validation accuracy of the training epochs\n",
      " 243:     validation_acc = np.amax(history.history['val_acc']) \n",
      " 244:     print('Best validation acc of epoch:', validation_acc)\n",
      " 245:     return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n",
      " 246: \n",
      "Beginning loading\n",
      "Finished loading, beginning X_train axis adjustment\n",
      "Beginning y_train encoding\n",
      "Data is NOT randomized.\n",
      "Beginning data splitting\n",
      "(433563, 1, 15, 100) (108391, 1, 15, 100) (433563,) (108391,) (433563,) (108391,)\n",
      "  0%|          | 0/5 [00:00<?, ?it/s, best loss: ?]WARNING:tensorflow:From /home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:245: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:184: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "                                                   \n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "                                                   \n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "                                                   \n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00013: early stopping                        \n",
      "Best validation acc of epoch:                      \n",
      "0.745809153949332                                  \n",
      "                                                                            \n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "                                                                            \n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "                                                                            \n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Restoring model weights from the end of the best epoch                      \n",
      "Epoch 00014: early stopping                                                 \n",
      "Best validation acc of epoch:                                               \n",
      "0.7696949007840179                                                          \n",
      "                                                                             \n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "                                                                             \n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "                                                                             \n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Restoring model weights from the end of the best epoch                       \n",
      "Epoch 00013: early stopping                                                  \n",
      "Best validation acc of epoch:                                                \n",
      "0.7694365768755037                                                           \n",
      "                                                                             \n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "                                                                             \n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "                                                                             \n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Restoring model weights from the end of the best epoch                       \n",
      "Epoch 00014: early stopping                                                  \n",
      "Best validation acc of epoch:                                                \n",
      "0.7589375502629372                                                           \n",
      "                                                                             \n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "                                                                             \n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "                                                                             \n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Restoring model weights from the end of the best epoch                       \n",
      "Epoch 00011: early stopping                                                  \n",
      "Best validation acc of epoch:                                                \n",
      "0.7035639491122458                                                           \n",
      "100%|██████████| 5/5 [07:40<00:00, 92.13s/it, best loss: -0.7696949007840179]\n"
     ]
    }
   ],
   "source": [
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=5,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name = 'HyperparamOpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning loading\n",
      "Finished loading, beginning X_train axis adjustment\n",
      "Beginning y_train encoding\n",
      "Data is NOT randomized.\n",
      "Beginning data splitting\n",
      "(1036381, 1, 15, 100) (259096, 1, 15, 100) (1036381,) (259096,) (1036381,) (259096,)\n",
      "Evalutation of best performing model:\n",
      "259096/259096 [==============================] - 38s 148us/step\n",
      "[0.6197369319482279, 0.7933584462901782]\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'filter_len': 1, 'l2': 0, 'num_blocks': 2, 'num_blocks_1': 0, 'num_dense': 1}\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(X_test, Y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 1:\n",
    "'l2': hp.choice('l2', [0.001, 0.01, 0.05]),\n",
    "'filter_len': hp.choice('filter_len', [3, 5]),\n",
    "'num_blocks': hp.choice('num_blocks', [2, 3, 4, 5]),\n",
    "'num_blocks_1': hp.choice('num_blocks_1', [2, 4]),\n",
    "'num_dense': hp.choice('num_dense', [1, 2]),\n",
    "\n",
    "Best performing model chosen hyper-parameters:\n",
    "{'filter_len': 1, 'l2': 0, 'num_blocks': 2, 'num_blocks_1': 0, 'num_dense': 1}\n",
    "\n",
    "Filter length = 5\n",
    "l2 = 0.001\n",
    "num_blocks = 4\n",
    "maxpool_factor = 2\n",
    "num_dense_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
