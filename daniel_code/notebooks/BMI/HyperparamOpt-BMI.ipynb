{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning notebook with Hyperas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape, Input, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(42)\n",
    "\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    \"\"\"\n",
    "    Data providing function:\n",
    "\n",
    "    This function is separated from create_model() so that hyperopt\n",
    "    won't reload data for each evaluation run.\n",
    "    \"\"\"\n",
    "    print('Beginning loading')\n",
    "\n",
    "    X_train = np.load('../processed_datasets/BMI_X_15_xtremefiltered.npy')\n",
    "    y_train = np.load('../processed_datasets/BMI_y_15_xtremefiltered.npy')\n",
    "    ids_train = np.load('../processed_datasets/BMI_ids_15_xtremefiltered.npy')\n",
    "\n",
    "    print('Finished loading, beginning X_train axis adjustment')\n",
    "\n",
    "    #Change X_train to weird format\n",
    "    X_train = np.expand_dims(X_train, 1)\n",
    "    X_train = np.swapaxes(X_train, -1, -2)\n",
    "\n",
    "    #Randomize\n",
    "    if False:\n",
    "        print('Data will be randomized.')\n",
    "        idxs = np.arange(X_train.shape[0])\n",
    "        np.random.shuffle(idxs)\n",
    "        X_train = X_train[idxs]\n",
    "        y_train = y_train[idxs]\n",
    "        ids_train = ids_train[idxs]\n",
    "    else:\n",
    "        print('Data is NOT randomized.')\n",
    "\n",
    "    print('Beginning data splitting')\n",
    "\n",
    "    split_num = int(0.8*X_train.shape[0])\n",
    "    X_test = X_train[split_num:]\n",
    "    y_test = y_train[split_num:]\n",
    "    ids_test = ids_train[split_num:]\n",
    "\n",
    "    X_train = X_train[:split_num]\n",
    "    y_train = y_train[:split_num]\n",
    "    ids_train = ids_train[:split_num]\n",
    "    \n",
    "    print(X_train.shape, X_test.shape, y_train.shape, y_test.shape, ids_train.shape, ids_test.shape)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Model providing function:\n",
    "\n",
    "    Create Keras model with double curly brackets dropped-in as needed.\n",
    "    Return value has to be a valid python dictionary with two customary keys:\n",
    "        - loss: Specify a numeric evaluation metric to be minimized\n",
    "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "    The last one is optional, though recommended, namely:\n",
    "        - model: specify the model just created so that we can later use it again.\n",
    "    \"\"\"\n",
    "    \n",
    "    #VGG\n",
    "    import functools\n",
    "    import json\n",
    "\n",
    "    import keras.backend as K\n",
    "\n",
    "    from keras.layers import Permute, Dense, Input, Conv2D, concatenate, MaxPooling2D\n",
    "    from keras.layers import ELU, BatchNormalization, Dropout, GlobalAveragePooling2D\n",
    "    from keras.models import Model\n",
    "\n",
    "    # %load model_util\n",
    "    from keras.layers import Input\n",
    "    from keras.regularizers import l2\n",
    "\n",
    "\n",
    "    REG_P = dict(kernel_regularizer=l2({{choice([0.001, 0.01, 0.05])}}))\n",
    "\n",
    "\n",
    "    def _create_input(input_shape, input_tensor=None, name='input'):\n",
    "        \"\"\"\n",
    "        Select a correct input tensor based on shape and instance specification.\n",
    "\n",
    "        # Arguments\n",
    "            input_shape: Input shape tuple\n",
    "            input_tensor: Existing tensor to wrap into the `Input` layer.\n",
    "                          If set, the layer will not create a placeholder tensor.\n",
    "            name: Name string for layer.\n",
    "\n",
    "        # Returns\n",
    "            Input Tensor\n",
    "        \"\"\"\n",
    "        if input_tensor is None:\n",
    "            return Input(shape=input_shape, name=name)\n",
    "\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            return Input(tensor=input_tensor, shape=input_shape, name=name)\n",
    "\n",
    "        return input_tensor\n",
    "\n",
    "\n",
    "    def _conv_block(units, block=1, layer=1, sensor='acc'):\n",
    "        \"\"\"\n",
    "        Create VGG style convolutional block.\n",
    "\n",
    "        Deviations from original paper.\n",
    "            - Remove `Dropout`\n",
    "            - Added `BatchNormalization`\n",
    "            - He-normal initialization\n",
    "            - Uses `ELU` Activation\n",
    "\n",
    "        # Arguments\n",
    "            units: conv filters\n",
    "            block: block number within network (used for naming)\n",
    "            layer: layer number within block (used for naming)\n",
    "            sensor: sensor name (used for naming)\n",
    "\n",
    "        # Returns\n",
    "            callable\n",
    "        \"\"\"\n",
    "        def layer_wrapper(inp):\n",
    "            filter_len = {{choice([3, 5])}}\n",
    "            x = Conv2D(units, (1, filter_len), padding='same', kernel_initializer='he_normal',\n",
    "                       name=f'block{block}_conv{layer}_{sensor}', **REG_P)(inp)\n",
    "            x = BatchNormalization(name=f'block{block}_bn{layer}_{sensor}')(x)\n",
    "            x = ELU(name=f'block{block}_act{layer}_{sensor}')(x)\n",
    "            return x\n",
    "\n",
    "        return layer_wrapper\n",
    "\n",
    "\n",
    "    def _dense_block(units, dropout=0.3, name='fc1'):\n",
    "        \"\"\"\n",
    "        Create VGG fully connected block.\n",
    "\n",
    "        # Deviations from original paper.\n",
    "            - Added `BatchNormalization`\n",
    "            - Uses `ELU` Activation\n",
    "\n",
    "        # Arguments\n",
    "            units: fc layer dimensionality\n",
    "            dropout: dropout probability\n",
    "            name: prefix for dense layers\n",
    "\n",
    "        # Returns\n",
    "            callable\n",
    "        \"\"\"\n",
    "\n",
    "        def layer_wrapper(inp):\n",
    "            x = Dense(units, name=f'{name}', **REG_P)(inp)\n",
    "            x = BatchNormalization(name=f'{name}_bn')(x)\n",
    "            x = ELU(name=f'{name}_act')(x)\n",
    "            x = Dropout(dropout, name=f'{name}_dropout')(x)\n",
    "            return x\n",
    "\n",
    "        return layer_wrapper\n",
    "\n",
    "\n",
    "    def _vgg_body(factor=4, sensor='acc'):\n",
    "        \"\"\"\n",
    "        VGG Network Body containing convolutional blocks\n",
    "\n",
    "        # Arguments\n",
    "            factor: scaling factor to reduce network filter width\n",
    "            sensor: sensor name\n",
    "\n",
    "        # Return\n",
    "            callable\n",
    "        \"\"\"\n",
    "\n",
    "        _vgg_conv_block = functools.partial(_conv_block, sensor=sensor)\n",
    "\n",
    "        def layer_wrapper(inp):\n",
    "            x = Permute((1, 3, 2), name=f'swapaxes_{sensor}')(inp)\n",
    "            \n",
    "            num_blocks = {{choice([2, 3, 4, 5])}}\n",
    "            \n",
    "            for block_num in range(1, num_blocks + 1):\n",
    "\n",
    "                x = _vgg_conv_block(32 * (2**block_num) // factor, block=block_num, layer=1)(x)\n",
    "                x = _vgg_conv_block(32 * (2**block_num) // factor, block=block_num, layer=2)(x)\n",
    "                x = MaxPooling2D((1, {{choice([2, 4])}}), name=f'block{block_num}_pool_{sensor}')(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "        return layer_wrapper\n",
    "\n",
    "    def VGG16Net(input_shape=None, input_tensor=(None, None),\n",
    "                 classes=1000, dropout=0.3, factor=2):\n",
    "        \"\"\"\n",
    "        Modified VGG architecture\n",
    "            https://arxiv.org/abs/1409.1556\n",
    "\n",
    "        # Arguments\n",
    "            input_shape: shape tuple\n",
    "            input_tensor: Keras tensor (i.e. output of `layers.Input()`) to use as image input for the model.\n",
    "            classes: optional number of classes to classify images\n",
    "            dropout: dropout applied to fc layers\n",
    "            factor: scaling factor to reduce network filter width\n",
    "\n",
    "        # Returns\n",
    "            A Keras model instance.\n",
    "        \"\"\"\n",
    "        assert input_shape or all(input_tensor), f'Must provide at least one: input_shape, input_tensor'\n",
    "\n",
    "        # Two Inputs\n",
    "        img_input_acc = _create_input(input_shape, input_tensor=input_tensor[0], name='acc_input')\n",
    "\n",
    "        # Accelerometer and Gyroscope Conv Blocks\n",
    "        x = _vgg_body(factor=factor, sensor='acc')(img_input_acc)\n",
    "\n",
    "        # Merge and Pool Channels\n",
    "        x = GlobalAveragePooling2D(name='avgpool')(x)\n",
    "\n",
    "        # FC Layers\n",
    "        num_dense = {{choice([1, 2])}}\n",
    "        for i in range(num_dense):\n",
    "            x = _dense_block(4098 // factor, dropout=dropout, name=f'fc{i + 1}')(x)\n",
    "\n",
    "        # Classification block\n",
    "        x = Dense(1, name='predictions')(x)\n",
    "\n",
    "        return Model(img_input_acc, x, name='VGG16Net')\n",
    "    \n",
    "    model = VGG16Net(input_shape=(1, 15, 100), classes=2)\n",
    "\n",
    "    #Define optimizer\n",
    "    adam = keras.optimizers.Adam() #Default lr is 0.001\n",
    "\n",
    "    model_metrics = ['mse']\n",
    "\n",
    "    #Training parameters\n",
    "    batch_size = 512\n",
    "    \n",
    "    model.compile(loss='mse',\n",
    "                  optimizer=adam,\n",
    "                  metrics=model_metrics)\n",
    "\n",
    "    #Callbacks\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                  patience=2, min_lr=1e-7, verbose=1)\n",
    "\n",
    "    early_stop = EarlyStopping(patience=4, verbose=1, restore_best_weights=True)\n",
    "\n",
    "    #tb = TensorBoard(log_dir=os.path.join(output_dir, 'logs'), \n",
    "    #                 write_graph=False,) \n",
    "                     #histogram_freq=5, \n",
    "                     #embeddings_freq=5, \n",
    "                     #embeddings_layer_names=['fc1'],\n",
    "                     #mbeddings_data = X_val)\n",
    "\n",
    "    cb_list = [reduce_lr, early_stop]\n",
    "\n",
    "    #Get data and train\n",
    "    #Use a generator for smaller epochs\n",
    "    class DataGenerator(keras.utils.Sequence):\n",
    "        'Generates data for Keras'\n",
    "        def __init__(self, X, y, batch_size=512, shuffle=True):\n",
    "            self.batch_size = batch_size\n",
    "            self.y = y\n",
    "            self.X = X\n",
    "            self.shuffle = shuffle\n",
    "            self.on_epoch_end()\n",
    "\n",
    "        def __len__(self):\n",
    "            'Denotes the number of batches per epoch'\n",
    "            return int(np.floor(len(self.y) / self.batch_size))\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            'Generate one batch of data'\n",
    "            idxs = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "            # Generate the batch\n",
    "            X = self.X[idxs]\n",
    "            y = self.y[idxs]\n",
    "\n",
    "            return X, y\n",
    "\n",
    "        def on_epoch_end(self):\n",
    "            'Updates indexes after each epoch'\n",
    "            self.indexes = np.arange(len(self.X))\n",
    "            if self.shuffle == True:\n",
    "                np.random.shuffle(self.indexes)\n",
    "    train_gen = DataGenerator(X_train, y_train)\n",
    "\n",
    "    history = model.fit_generator(train_gen,\n",
    "                        steps_per_epoch = 200,\n",
    "                        epochs = 200,\n",
    "                        validation_data = (X_test, y_test),#.squeeze().swapaxes(-1, -2), y_val),\n",
    "                        callbacks = cb_list,\n",
    "                        class_weight={0: 1.5, 1: 0.5}, #class_weights,\n",
    "                        shuffle=True,\n",
    "                        verbose=0)\n",
    "    \n",
    "    #get the highest validation accuracy of the training epochs\n",
    "    validation_acc = np.amax(history.history['val_acc']) \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<unknown>, line 357)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/users/danjwu/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3296\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-7-fee35502b676>\"\u001b[0m, line \u001b[1;32m6\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    notebook_name = 'HyperparamOpt-BMI')\n",
      "  File \u001b[1;32m\"/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/hyperas/optim.py\"\u001b[0m, line \u001b[1;32m69\u001b[0m, in \u001b[1;35mminimize\u001b[0m\n    keep_temp=keep_temp)\n",
      "  File \u001b[1;32m\"/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/hyperas/optim.py\"\u001b[0m, line \u001b[1;32m98\u001b[0m, in \u001b[1;35mbase_minimizer\u001b[0m\n    model_str = get_hyperopt_model_string(model, data, functions, notebook_name, verbose, stack)\n",
      "  File \u001b[1;32m\"/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/hyperas/optim.py\"\u001b[0m, line \u001b[1;32m189\u001b[0m, in \u001b[1;35mget_hyperopt_model_string\u001b[0m\n    imports = extract_imports(cleaned_source, verbose)\n",
      "  File \u001b[1;32m\"/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/site-packages/hyperas/utils.py\"\u001b[0m, line \u001b[1;32m40\u001b[0m, in \u001b[1;35mextract_imports\u001b[0m\n    tree = ast.parse(source)\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/groups/euan/users/danjwu/miniconda3/envs/MHC/lib/python3.6/ast.py\"\u001b[0;36m, line \u001b[0;32m35\u001b[0;36m, in \u001b[0;35mparse\u001b[0;36m\u001b[0m\n\u001b[0;31m    return compile(source, filename, mode, PyCF_ONLY_AST)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<unknown>\"\u001b[0;36m, line \u001b[0;32m357\u001b[0m\n\u001b[0;31m    Experiment 1:\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=5,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name = 'HyperparamOpt-BMI')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(X_test, Y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
